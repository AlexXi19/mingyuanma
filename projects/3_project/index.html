<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Formalness | Mingyuan  Ma</title>
    <meta name="author" content="Mingyuan  Ma">
    <meta name="description" content="Evaluate formal expression from 1000+ texts from Reddit using BERT-classifier and LogReg">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://thunderbeee.github.io/projects/3_project/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mingyuan </span>Ma</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">Main</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">GitHub</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Formalness</h1>
            <p class="post-description">Evaluate formal expression from 1000+ texts from Reddit using BERT-classifier and LogReg</p>
          </header>

          <article>
            <h3 id="introduction"><strong>Introduction</strong></h3>

<p>The most exciting applications of NLP haven’t been invented yet. While much of this course will give you exposure to the common methods in NLP, you will also carry out an annotation project where you will get exposure to the entire NLP design process for building a classifer for a brand new task. You will decide on a new NLP task — either document classification (one label per document) or sequence labeling (one label per contiguous span) — annotate data to support it (including creating annotation guidelines), measure your inter-annotator agreement rate, and build a classifier to predict those labels using the methods we discuss in class (<a href="https://people.ischool.berkeley.edu/~dbamman/nlp22.html" rel="external nofollow noopener" target="_blank">INFO 159, spring 2022</a>). This is a group project, where I worked with my teammates, Yuanrui Zhu and Shuyao Zhou</p>

<p>In our AP project, the category we are annotating is a Reddit comment “casual”, “medium”, or “formal”. What we want to do is to subjectively map each comment to a numerical value 1~3, where 1 refers to “casual”, 2 refers to “medium”, and 3 refers to “formal.”</p>

<p>The tentative guideline to give those scores (categories) is ceiling round up average score of three parts (subjective and may be more parts later), and each part would be rated 1~3. First, “Word” which based on the words they use (proportion of colloquial words, and the lexical level of the sentence); second, “structure” which based on the punctuations and the complexity of the sentence (“!” more likely to be casual and the more complex the sentence is the higher score it would get); third is the “character” of one sentence (eg. “I” would be personal which is less formal than “the author”).</p>

<h3 id="annotation-guideline"><strong>Annotation Guideline</strong></h3>

<p>we have uploaded our final annotation guideline to <a href="https://github.com/Thunderbeee/NLP-attitude-score" rel="external nofollow noopener" target="_blank">GitHub</a></p>

<h3 id="data"><strong>Data</strong></h3>

<p>we get data from Reddit and clean @ <a href="https://github.com/Thunderbeee/NLP-attitude-score" rel="external nofollow noopener" target="_blank">GitHub</a></p>

<h3 id="modeling"><strong>Modeling</strong></h3>

<p>we use BERT-classifier and Logistic Regression (LogReg) in classification tasks</p>

<h3 id="analysis"><strong>Analysis</strong></h3>

<p><strong>Label Mistaken Identification</strong></p>

<p>We plotted the confusion matrix comparing each class of actual and predicted test results, and accordingly calculated the Precision, Recall, and F-1 score for each of the four classes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
For Class 0 (Casual), precision = 0.632, recall = 0.753, F1 = 0.687
For Class 1 (Ordinary), precision = 0.379, recall = 0.2, F1 = 0.262
For Class 2 (Formal), precision = 0.569, recall = 0.649, F1 = 0.607
For Class 3 (Artistic), precision = recall = F1= 0
---
</code></pre></div></div>

<p>Analysis of statistics:</p>

<ul>
  <li>For each class, its precision and recall are close. That is, for a class with high precision,
its recall is also high. So, clearly, there is a performance gap among the classes.</li>
  <li>The performance of Casual and Formal is much better than Ordinary, which follows the general case that the Ordinary class is a lot more difficult to classify than the other two classes. However, when putting a text to be in Class 1 (Ordinary), we may involve our personal judgment based on our unique educational and multilingual background. Hence, we’re not classifying based on the same metrics as machines, and it’s normal for
algorithms not to capture these factors and performing badly.</li>
  <li>The score of Casual is the highest among all the classes, which is largely due to the
apparent feature it demonstrates to readers (as we stated in the guideline).</li>
  <li>We observed that the logistic model tends to classify real Class 1 to Class 0/Class 2 since the precision for Class 1 is greater than its recall, which is the opposite case for Class 0 and Class 2. Given for the recall, real Y is in the denominator position, we can infer
Class 1 has nearly no apparent features that can be relied on.</li>
  <li>Because of the uneven nature of our dataset, precision and recall are all zero for Class 3,
which we will conduct further discussion in the following parts.</li>
</ul>

<p>From the data set, we can clearly conclude that Class 1 (Ordinary) is often mistaken for Class 0 (Casual). Let’s see why this would happen by looking at the example texts:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/nlp/pic2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/nlp/pic2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/nlp/pic2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/nlp/pic2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<p>There are fewer cases of the opposite, where the prediction is Class 1 when the actual category is Class 0:</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/nlp/pic3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/nlp/pic3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/nlp/pic3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/nlp/pic3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<p>Looking closely at the misclassification cases, we can tell that they’re actually closer to Class 1 than we expected them to be. From the perspective of speakers, the person who says ‘Next level reminds me of extreme movies’ and ‘Truelit and Truefilm are good though’ may very likely be in a casual state. We may have the wrong sense of those texts at the very beginning since we didn’t quite get what the writer really wanted to express.</p>

<p>Beyond that, we would modify our guidelines boundary based on our observations. There’re some sentences we would identify as “plain text”, such as “I use a combination of Excel and Goodreads.” We originally defined such “plain text” to be ordinary, but now we would choose to categorize them as Casual rather than Ordinary. We believe that besides “sentence structure”, “sentence content” is another critical measurement of casualness. If the author explains a preference or a quick fact without any detailed discussion, this should be categorized as casualness of “sentence content”, and thus it is casual overall.</p>

<p>One flaw of the training set and testing set is the imbalance of our data. Class 3 (Artistic) is rare, which puts some limitations on training, thus pushing the algorithm to ignore the underrepresented class entirely. Also, literary devices are very literal and are hard to learn from a small set of data. This explains why none of the three artistic labels in the test set are classified correctly. We should include more data in the training set to capture the internal patterns and rhetorical devices of the artistic paragraphs. Firstly, we get the percentage of each Class in the training set:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
Class 0 proportion (prior): 40.7%
Class 1 proportion (prior): 26.83%
Class 2 proportion (prior): 31.33%
Class 3 proportion (prior): 1.17% 
---
</code></pre></div></div>

<p>At this point, one way to deal with this problem is randomly oversampling Class 3. We modified the classifier so it over-samples the underrepresented class and makes it approximately equal to the size of the most prevalent class. Now we reach the following accuracy:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
Class 0 proportion (posterior): 29.15%
Class 1 proportion (posterior): 19.24%
Class 2 proportion (posterior): 22.46%
Class 3 proportion (posterior): 29.15%
---
</code></pre></div></div>

<p><strong>Features Learned to most define the Class (logistic regression)</strong></p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/nlp/pic6-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/nlp/pic6-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/nlp/pic6-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/nlp/pic6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<p>We will explain the strength of some features. For example, “!” / “?” are signs of strong sentiment, and “me” / “my” are signs of personal expression, which as we defined in the guideline: Are indicators of casualness. Furthermore, “as’’ can be interpreted as “reasoning” and make sense to be a sign of Class 2. More “.” in a document indicates that it is a longer text, which would be more likely to have detailed explanations and thus higher probability of formalness. Although Class 2 has the worst performance among all the classes, the defining features of Class 2 are still quite intuitive. “Dark”, “world”, and “love” are general intangible words that aren’t usually used in formal expressions.</p>

<p>However, there are plenty of features that we wouldn’t intuitively interpret as their strength. “Are” is a singular plural present tense of “be” which seems very prevalent in every kind of text in the real world, but here we only have it as a strong sign of formalness.</p>

<p>From the feature above, we also see something we didn’t carefully consider when selecting the data. We notice that for Class 1, “book”, “literature”, and “r/book” are among the top features of predictive power. This means there is a bias that the post we selected from the r/book thread is full of ordinary posts. Similarly, “r/truelit” and “r/askliterary studies’ ‘have a large proportion of formal posts, which makes them the defining feature of Class 2. This phenomenon is against our initial idea of only focusing on the text itself in order to grasp the feature of casualness-formalness. We believe changing the span of posts we analyze (also maybe removing any tags that exist in the post) can solve this issue to some extent.</p>

<h5 id="we-also-tried-to-run-data-on-bert-and-made-some-observations">We also tried to run data on BERT and made some observations:</h5>

<p><em>For keeping consistency with the BERT Jupyter Notebook, we will use Class 1 as Casual, Class 2 as Ordinary, Class 0 as Formal, and Class 3 as Artistic throughout this part. Note that this is different from our guidelines.</em></p>

<p>Not satisfying with the result from logistic regression with self-defined feature above, we want to see if the deep learning model, BERT, considering the dependency of each words in one document and view the document as a whole, would perform better than the logistic regression which is feature-based using n-grams. Due to the GPU capacity issue of Google Colab, we changed the BERT model to “google/bert_uncased_L-2_H-128_A-2”. After 25 Epoch, BERT achieved the test accuracy for best dev model: 0.610 with 95% Confidence Interval [0.542 0.678]. We can find that BERT performs better than the previous logistic regression. Although our self-defined features in the logistic regression model are the optimal ones after doing several trials, the BERT model can perform better than the logistic regression. This outstanding performance makes us understand the importance of viewing the document holistically when doing subjective judgment tasks. We should not emphasize certain words or certain pairs of words. Instead, as humans, we judge a sentence after reading it from start to the end. BoW or other n-grams cannot capture such nuance in sentences; otherwise, as what we mentioned above, it may overfit the data. So it is not surprising that BERT performs better: it works more similarly to how human perform classification tasks than n-grams. Although there is no feature in the deep learning model, BERT, we still want to delve into those misclassifications to see if we can observe any pattern.
We then generated a dictionary (key is the text, value is the (test_label, our_label) tuple pair) to compare the discrepancy between the test labels and our labels. Most of the differences are (1, 2) and (2, 0). We can know that Class 1 (Casual) is often mistaken for Class 2 (Ordinary). Also, Class 2 is often mistaken for Class 0 (Formal). This is expected because the boundaries between Class 2 and other classes were not strict. After examining the texts, we can tell that BERT has some biases. BERT would surprisingly classify some documents incorrectly.
12
For example, “Always happy to hear more people chime in! I know a big thing for all the contributors is knowing that people are actually reading and using posts/booklists/work. A little thank you goes a long way!Always happy to hear suggestions too. <em>Generally</em> we try to keep the bulk of recommendations from flaired members because it gives us a good idea of who’s expertise is suggesting it. Not <em>super</em> sure what other avenues for volunteer work there is, other then as your using it let us know if you come across any problems. Broken links, typos, stuff like that.\n”. BERT classifies it as Class 2 whereas we classified it as Class 1. It is surprising that BERT would misclassify this text. Obviously it would be Class 1 as there we can find informal words and exclamations which show the casual emotions. Such a basic classification task would be successfully done by a simple model, such as logistic regression whereas BERT may fail classifying it. Another phenomenon that BERT fails to consider is the group of internet slangs. This is also expected because internet slangs update very fast and hard to predict one from others. Also, BERT may not capture it correctly as a sign/feature of one class. However, we still believe that BERT would perform better if we have a more complex structure to capture those details and gain more useful information from documents.</p>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Mingyuan  Ma. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Great thanks to <a href="https://www.alexhxi.com" target="_blank" rel="external nofollow noopener">Alex Xi</a>

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
